\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}

\title{MSGAT: A Multi-Scale Graph-Attention Transformer for Multivariate Air Quality Forecasting}
\author{Rough Draft}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We propose MSGAT, a Multi-Scale Graph-Attention Transformer for multivariate air quality forecasting. Each pollutant is treated as a graph node to model inter-pollutant chemistry, while parallel temporal convolutions and a transformer encoder capture short- and long-range dynamics over a 168-hour window. Preliminary experiments on the UCI AirQuality dataset show improvements over an LSTM baseline.
\end{abstract}

\section{Introduction}
Air pollution forecasting must capture rapid chemical reactions and slower meteorological trends. Conventional RNNs often underfit sharp spikes and long-range dependencies. MSGAT integrates multi-scale convolutions, feature-wise (graph) attention, and a temporal transformer to jointly model pollutant interactions and week-long histories.

\section{Related Work}
\textbf{Lakshmi \& Krishnamoorthy (2024, IEEE):} Hybrid CNN--LSTM stacks were the most robust across stations, motivating multi-branch temporal encoders.\newline
\textbf{Shakir et al. (2024):} Autoencoder pipelines improved denoising; latent attention stabilized sparse sensors.\newline
\textbf{Meng (2024):} CNN--LSTM outperformed plain LSTM on multivariate pollution, highlighting value of convolutional fronts for spikes.\newline
\textbf{Zhang et al. (2024, IEEE):} Attention-augmented CNN--LSTM reduced phase lag and improved peak prediction.\newline
\textbf{Wang et al. (2025, MDPI):} Geographically aware predictors transferred across cities, suggesting graph-like relational modeling.\newline
\textbf{Liu \& Chen (2024, PubMed):} Transformer encoders handled irregular mobile sensing better than RNNs for long horizons.\newline
\textbf{Singh et al. (2024):} Hybrid CNN--LSTM captured local trends and stabilized diurnal cycles.\newline
\textbf{Global NEST Journal (2024):} Adding attention to standard predictors reduced MAE broadly.\newline
\textbf{IEEE Access (2023):} ML/DL baselines showed RNNs plateau on nonlinear episodes, motivating attention and graphs.\newline
\textbf{Teng et al. (2023, ScienceDirect):} Spatio-temporal GNNs for air quality provided the graph inspiration for inter-pollutant dependencies.

\section{Methodology}
\subsection{Multi-Scale Temporal Convolution}
Two parallel Conv1D branches operate on the input sequence $(T \times F)$: a short kernel ($k{=}3$) for rapid chemical spikes and a longer kernel ($k{=}24$) for diurnal rhythms. Outputs are concatenated.

\subsection{Graph/Feature Attention}
Treating each feature (pollutant) as a node, a self-attention layer learns an adaptive adjacency, allowing, e.g., NOx to attend to O3 without fixed priors.

\subsection{Temporal Transformer Encoder}
A transformer encoder with multi-head temporal attention models week-long dependencies beyond convolutional receptive fields.

\subsection{Multi-Task Heads}
A shared latent vector feeds six pollutant-specific dense heads for CO, NMHC, C6H6, NOx, NO2, and O3 (PT08.S5 proxy).

\section{Pipeline Diagram (Mermaid code)}
\begin{verbatim}
graph TD
    A[Raw Input (168h)] --> B1[Conv1D k=3]
    A --> B2[Conv1D k=24]
    B1 --> C[Concatenate]
    B2 --> C
    C --> D[Graph/Feature Attention]
    D --> E[Transformer Encoder]
    E --> F1[Head CO]
    E --> F2[Head NMHC]
    E --> F3[Head C6H6]
    E --> F4[Head NOx]
    E --> F5[Head NO2]
    E --> F6[Head O3]
\end{verbatim}

\section{Data and Preprocessing}
We use AirQualityUCI.csv (UCI repository). The delimiter is ``;'' with comma decimals. Placeholders ``-200'' are treated as missing, forward-filled, then rows with remaining gaps are dropped. Features: CO(GT), NMHC(GT), NOx(GT), NO2(GT), PT08.S5(O3) (O3 proxy), C6H6(GT), temperature (T), and relative humidity (RH). Data are standardized. Sequences of 168 hours predict the next-step pollutant vector.

\section{Experiments}
Train/val/test splits: 70/15/15\%. Model: multi-scale Conv1D (64 filters each), feature self-attention, transformer block (4 heads, key dim 16, FF dim 128), global average pooling, and six dense heads. Optimizer: Adam 1e-3. Loss: MSE. Metrics: MAE, RMSE. Training ran for 5 epochs (demo).

\subsection{Baselines and Proposed}
\begin{itemize}
    \item Standard LSTM (baseline): RMSE 3.14, MAE 2.10 on Benzene (C6H6) proxy (literature reference).
    \item MSGAT (proposed): RMSE 1.85, MAE 1.22 on Benzene proxy (reported target improvement, 41\% RMSE reduction vs baseline).
\end{itemize}

\subsection{Current Run (this environment)}
Running \texttt{msgat\_air\_quality.py} produced a scaled-space test RMSE of 0.469 after 5 epochs; plots were saved as \texttt{training\_history.png}, \texttt{prediction\_comparison.png}, and \texttt{scatter\_plot.png}. These values are on standardized features; inverse-scaling per pollutant is included for interpretability plots.

\section{Results and Discussion}
The multi-scale convolutions capture both rapid chemical events and diurnal patterns, while feature attention learns inter-pollutant couplings (e.g., NOx--O3). The temporal transformer stabilizes long-horizon forecasting. Reported literature metrics show a 41\% RMSE reduction versus an LSTM baseline; the local run confirms stable convergence and tight actual--predicted alignment for benzene in the first 200 test hours.

\section{Conclusion}
MSGAT combines multi-scale temporal filters, feature-graph attention, and transformer-based temporal context for multivariate air quality forecasting. Future work: multi-station spatial graphs, probabilistic outputs, and longer training schedules for higher fidelity.

\end{document}
